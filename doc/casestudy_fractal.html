<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>Jsonnet - The Data Templating Language</title>
    <meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
    <meta name="keywords" content="Jsonnet, JSON, YAML, language, configuration, configuration language, functional, declarative, lazy, structured, elegant, semantics, clean, mixins, inheritance, template, expansion, expand" />
    <meta name="description" content="The Jsonnet language allows elegant and easy description of JSON data." />

    <link rel="icon" type="image/png" href="favicon.png" />

    <link rel="stylesheet" href="prism.css" />
    <link rel="stylesheet" type="text/css" href="doc.css" />

    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-53570216-1', 'auto');
      ga('send', 'pageview');

    </script>

    <script src="prism.js" type="text/javascript">
    </script>

    <script type="text/javascript">
    Prism.languages.jsonnet = {
      'cppcomment': /\/\/.*/g,
      'comment': /\/\*[\w\W]*?\*\//g,
      'string': /("|')(\\?.)*?\1/g,
      'keyword': /\b(self|super)\b|\$/g,
      'boolean': /\b(true|false)\b/g,
      'constant': /\bnull\b/g,
      'error': /\berror\b/g,
      'special': /\b(local|function|if|then|else|import|importstr|for|in)\b/g,
      'number': /\b[0-9][0-9.eE]*\b/g,
      'operator': /(\+|\*|\/)+/g,
    };
    </script>

    <style type="text/css">
    .token.comment,
    .token.cppcomment {
      color: #040;
    }

    .token.identifier {
      color: #000;
    }
    .token.operator {
      color: #000;
    }

    .token.boolean,
    .token.number,
    .token.constant {
      color: #05a;
    }

    .token.string {
      color: #000048;
    }

    .token.keyword,
    .token.special {
      color: blue;
    }

    .token.error {
      color: red;
    }
    </style>


    <script type="text/javascript">

        var menu_timeout = null;

        function set_visible(menu, b)
        {
            var category = menu.children[0];
            var dropdown = menu.children[1];
            category.style['border-radius'] = b && dropdown != null ? '4px 4px 0px 0px' : '4px';
            if (dropdown != null) {
                dropdown.style.visibility = b ? 'visible' : 'hidden';
            }
        }

        function menu_open(el)
        {
            menu_close_all();
            var menu = el;
            while (menu.id === "") {
                menu = menu.parentNode;
            }
            set_visible(menu, true);
        }

        function menu_leave()
        {
            if (menu_timeout != null) {
                window.clearTimeout(menu_timeout);
            }
            menu_timeout = window.setTimeout(menu_close_all, 300);
        }

        function menu_close_all()
        {
            var mb = document.getElementById("menubar");
            if (mb == null) return;
            var menus = mb.children;
            for (var i = 0 ; i < menus.length ; ++i) {
                set_visible(menus[i], false);
            }
            if (menu_timeout != null) {
                window.clearTimeout(menu_timeout);
                window_timeout = null;
            }
        }

        document.onclick = menu_close_all(); 

    </script>

</head>

<body class="language-jsonnet">

<div class="header">

<div class="tagline"><i>The data templating language</i></div>
<div id="githubmarkbox">
<a href="https://groups.google.com/forum/#!forum/jsonnet"><div id="groupsmark"></div></a>
<a href="http://github.com/google/jsonnet"><div id="githubmark"></div></a>
</div>
<div class="title"><a href="index.html" class="title">Jsonnet</a></div>

<ol id="menubar">

    <li id="menu_home">
        <a href="index.html" onmouseover="menu_open(this)" onmouseout="menu_leave()">Home</a>
    </li>

    <li id="menu_userdocs">
        <a href="userdocs.html" onmouseover="menu_open(this)" onmouseout="menu_leave()">User docs</a>
        <div onmouseover="menu_open(this)" onmouseout="menu_leave()">
            <a href="tutorial.html">Tutorial</a>
            <a href="demo.html">Demo</a>
            <a href="stdlib.html">Standard Library</a>
        </div>
    </li>

    <li id="menu_language">
        <a href="language.html" onmouseover="menu_open(this)" onmouseout="menu_leave()">Language</a>
        <div onmouseover="menu_open(this)" onmouseout="menu_leave()">
            <a href="design.html">Design</a>
            <a href="comparisons.html">Comparisons</a>
            <a href="spec.html">Specification</a>
        </div>
    </li>

    <li id="menu_implementation">
        <a href="implementation.html" onmouseover="menu_open(this)" onmouseout="menu_leave()">Implementation</a>
        <div onmouseover="menu_open(this)" onmouseout="menu_leave()">
            <a href="commandline.html">Cmdline Tool</a>
            <a href="bindings.html">Libraries</a>
            <a href="cpp.html">C++ Internals</a>
            <a href="javascript.html">Javascript</a>
            <a href="tests.html">Tests</a>
        </div>
    </li>

    <li id="menu_contributing">
        <a href="contributing.html" onmouseover="menu_open(this)" onmouseout="menu_leave()">Contributing</a>
    </li>

</ol>

<div style="clear: both"></div>

</div>



<h1 id=top>Case Study: Cloud Application</h1>

<div id=contents>
    <div>
        <a href="#intro">Introduction</a>
    </div>
    <div>
        <a href="#app">Example Web Application</a>
    </div>
    <div>
        <a href="#config">Configuration Structure</a>
        <div><a href="#config_whirlwind">Whirlwind Tour</a></div>
    </div>
    <div>
        <a href="#using">Using The Configuration</a>
        <div><a href="#using_deploy">Initial Deployment and Tear Down</a></div>
        <div><a href="#using_cassandra">Add / Remove Cassandra Nodes</a></div>
        <div><a href="#using_appserv">Canary Change to Application Server</a></div>
    </div>

</div>

<div style="clear:both"></div>

<h2 id=intro>Introduction</h2>

<p> This case study illustrates that <a href="index.html">Jsonnet</a> can be used to centralize,
unify, and manage configuration for all parts of a cloud hosted multi-tier web application (a
Mandelbrot viewer).  Jsonnet centralizes configuration files for the various application software,
database schemas and initial data sets, system configuration files, package manifests, software
build configurations, image configurations (<a href="http://www.packer.io/">Packer</a>) and cloud
resources / connectivity (<a href="http://www.terraform.io/">Terraform</a>).  Although the running
example is deployed on <a href="https://cloud.google.com">Google Cloud Platform</a> and uses
specific application software, Jsonnet can be used to generate configuration for any application and
(via Packer &amp; Terraform) a wide range of cloud providers.</p>

<p>  Prerequisites: It is assumed that the reader has read the Jsonnet <a
href="tutorial.html">tutorial</a>, and has a basic knowledge of Packer and Terraform.</p>

<h2 id=app>Example Web Application</h2>

<a href="fractal_screenshot.png"><img src="fractal_screenshot.png" class=thumb></a>

<p>The example application allows the user to zoom and pan a Mandelbrot fractal (dynamically
rendered server side, in C++).  The user is able to
save the location of features they find, deep in the fractal, and a time-ordered list of these with
thumbnails is displayed in the left hand pane.  The application is provisionally hosted <a
href="http://fractal.noip.me/">here</a>, but you can easily deploy your own as all required files
are available in the Jsonnet repository.</p>

<p>Although admittedly a little contrived, this example is intended to represent the structure of a
typical non-trivial real-world web application.  It consists of 3 tiers (illustrated below):  An
application server, a backend database (Cassandra) and a backend service for generating the fractal
PNG tiles (C++).  Each tier is scalable and fault-tolerant.</p>

<img src="fractal_architecture.png">

<p>The application server hosts static content (CSS) and Jinja'd HTML.  The HTML <a
href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/appserv/templates/page.html">contains</a> Javascript that issues AJAX
requests to 1) the tile generation backend and 2) the application server (to fetch / amend the
discoveries list).  The application server therefore does not communicate directly with the fractal
tile generation service, but it needs to know the host:port endpoint in order to embed it in the
HTML so that the user's browser can do so.  The user does not communicate directly with the
Cassandra database.</p>

<p>Both the application server and the tile generation service use Nginx, uWSGI and flask to host
their content.  For the application server, this means transforming HTTP requests into database
accesses and/or serving content (code <a href="https://github.com/google/jsonnet/blob/master/case_studies/appserv/main.py">here</a>).  For the tile
generation service, this means invoking a compiled C++ <a
href="https://github.com/google/jsonnet/blob/master/case_studies/tilegen/mandelbrot.cpp">executable</a> from the Flask <a
href="https://github.com/google/jsonnet/blob/master/case_studies/tilegen/mandelbrot_service.py">handler</a> in order to construct a PNG for a given
tile / thumbnail of the fractal.  Both tiers consist of a group of <a
href="https://cloud.google.com/compute/docs/instances">instances</a> behind a <a
href="https://cloud.google.com/compute/docs/load-balancing/network/">layer 3 cloud load balancer</a>
with a static IP and a simple <a
href="https://cloud.google.com/compute/docs/load-balancing/health-checks">health check</a>.  The
Cassandra database is simply a set of instances, as the Cassandra client library (used by the
application server) does client-side load balancing and transparent failover, thus does not need a
cloud load balancer.</p>

<p>The application is deployed by first using Packer to build a image for each of the 3 kinds of
cloud instances (application server, fractal image processing service, Cassandra).  Then, all the
cloud resources (instances, load balancers, etc) are deployed using Terraform.  The Packer build
compiles, installs and configures all of the required software on each image.  The Terraform
configuration provides last minute configuration (host:port endpoints, passwords, etc) to the
instances via <a href="https://cloud.google.com/compute/docs/metadata">metadata</a>.</p>

<p>The choice about what configuration to provide at image build time (embedded in Packer
configurations) vs deployment time (embedded in Terraform configuration) is up to the user.  The
advantage of doing more at image build time is that instances can then be deployed more quickly
(useful in an auto-scaling situation).  But allowing some configuration at deployment time makes the
images more flexible.  Some configuration (e.g. host:port endpoints) is only known at deployment
time so must be specified in the Terraform configuration.  In our case, we try to do all time
consuming steps (downloading, generating, compiling) in Packer, while leaving finer details until
deployment.</p>


<h2 id=config>Configuration Structure</h2>

<p>This example is configured with a single Jsonnet <i>configuration</i>, which is logically
separated into several files via <code>import</code> constructs.  This is to promote abstraction
and re-usability, as well as having a separate credentials file (to avoid accidental checkin).  Note
that Jsonnet did not mandate this structure:  Other separations (or a giant single file) would also
have been possible.</p>

<p>This single configuration yields the a JSON packer configuration for each image (*.packer.json)
and the JSON Terraform configuration (terraform.tf), using <a href="commandline.html#multi">multiple
file output</a>.  Those configurations in turn embed other configurations for the application
software that will run on the instances.  The top-level structure of the generated configuration is
therefore as follows (with the content of each file elided for clarity).</p>

<pre class="medium"><code>{
    "appserv.packer.json": ...,
    "cassandra.packer.json": ...,
    "tilegen.packer.json": ...,
    "terraform.tf": ...
}
</code></pre>

<p>The configuration is built from the following input files:</p>











<ul>

<li><tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt>: The top level file that imports the others.  This is the filename that
is given to the jsonnet commandline utility.  In it are all the details that define the Fractal
example application, as a list of packer images and a Terraform configuration that refers to those
images.  Note the first few lines import the other Jsonnet files and store their contents in
top-level scoped variables (of the same names).  When we access things from those libraries (e.g.
templates to override) we will do so through the top-level variables.  Also note the use of
hidden fields (:: syntax) for things that are referenced within the structure but should not
appear in the output configuration.</li>

<li><tt>credentials.jsonnet</tt>: User and superuser keys for Cassandra, and Google Cloud Platform
project name.  You must create this file yourself, based on this <a
href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/credentials.jsonnet.TEMPLATE">template file</a>.</li>

<li><tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/packer.jsonnet">lib/packer.jsonnet</a></tt>: Some templates to help with writing Packer configurations.</li>

<li><tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/terraform.jsonnet">lib/terraform.jsonnet</a></tt>: Some templates to help with writing Terraform
configurations.</li>

<li><tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/cassandra.jsonnet">lib/cassandra.jsonnet</a></tt>: Some templates to help with writing Packer and Terraform
configurations for Cassandra.</li>

</ul>

<p>Note that the Jsonnet template libraries also include some definitions not used in this
example application, e.g.  PostgreSQL and MySQL templates.  Those can be ignored.</p>

<p>To integrate Jsonnet with Packer and Terraform, a <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/Makefile">Makefile</a></tt> is used.  This first runs
Jsonnet on the configuration and then runs Packer / Terraform on the resulting files (if they have
changed).  The choice of 'glue' tool is arbitrary, we could also have used a small Python script.
We chose make because it is well understood, available everywhere, and does 3 things that we want:
Invoke other programs, run things in parallel, and avoid repeating work that is already complete.
For example, Packer is only invoked if its configuration file has changed, and the 3 image builds
will proceed in parallel (they take a few minutes each).</p>

<p>Ignoring the re-usable templates, whitespace, and comments, the Jsonnet configuration is 217
lines (9.7kB).  The generated Packer and Terraform files are 740 lines (25kB) in total.  This
demonstrates the productivity benefits of using template expansion when writing configurations.</p>


<h3 id=config_whirlwind>Whirlwind Tour</h3>

<p> The fractal example is a complete realistic application and therefore its configuration has many
technical details.  In particular, it embeds configurations for various pieces of off-the-shelf
software that we don't want to go into in much depth.  However we would like to draw attention to
some particular uses of Jsonnet within the configuration.  We'll gladly field specific questions on
the <a href="https://groups.google.com/forum/#!forum/jsonnet">mailing list</a>.</p>


<h4>Packer And Application Configuration</h4>

<p>Each Packer configuration is, at its core, a list of imperative actions to be performed in
sequence by a VM, after which the disk is frozen to create the desired image.  The actions are
called <i>provisioners</i>.  Jsonnet is used to simplify the provisioner list by eliminating
duplication and encouraging abstraction.  Generic templates for specific off-the-shelf software are
defined in re-usable libraries, which are then referenced from <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> and, as
needed, overridden with some fractal application details.  Generic provisioners are also provided
for easy installation of packages via package managers, creation of specific files / dirs, etc.</p>

<p>In addition, the <code>ImageMixin</code> object in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> is used to factor out
common fractal-specific configuration from the 3 images.  This includes the Google Cloud Platform
project id and filename of the service account key.  Since all the images are derived ultimately
from the <code>GcpDebian</code> image (in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/packer.jsonnet">lib/packer.jsonnet</a></tt>), and this image includes apt
&amp; pip provisioners (discussed shortly), this is also a good place to ensure some basic packages
are installed on every image.  Note that since this object is not intended to be actually created
(it existing only for referencing from other parts of the configuration) it is stored in a hidden
field (:: syntax).</p>

<pre class="medium"><code>ImageMixin:: {
    project_id: credentials.project,
    account_file: "service_account_key.json",

    // For debugging:
    local network_debug = ["traceroute", "lsof", "iptraf", "tcpdump", "host", "dnsutils"],
    aptPackages +: ["vim", "git", "psmisc", "screen", "strace" ] + network_debug,
},
</code></pre>

<p>Both the application server's image configuration <tt>appserv.packer.json</tt> and the tile
generation service's image configuration <tt>tilegen.packer.json</tt> extend
<code>MyFlaskImage</code>, which exists merely to add the aforementioned <code>ImageMixin</code> to
the <code>GcpDebianNginxUwsgiFlaskImage</code> template from <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/packer.jsonnet">lib/packer.jsonnet</a></tt>.  That template
builds on the basic <code>GcpDebianImage</code> template from the same library, and adds all the
packages (and default configuration) for both Nginx and uWSGI.</p>

<pre class="medium"><code>MyFlaskImage:: packer.GcpDebianNginxUwsgiFlaskImage + $.ImageMixin,
</code></pre>

<p>In Jsonnet we use JSON as the canonical data model and convert to other formats as needed.  An
example of this is the uWSGI configuration, an <a
href="http://en.wikipedia.org/wiki/INI_file">INI</a> file, which specified in Jsonnet under the
<code>uwsgiConf</code> field in <code>GcpDebianNginxUwsgiFlaskImage</code>.  The JSON version is
converted to INI by the call to <code>std.manifestIni</code> (documented <a
href="stdlib.html">here</a>) in the provisioner immediately below it.  Representing the INI file
with the JSON object model (instead of as a string) allows elements of the uWSGI configuration (such
as the filename of the UNIX domain socket) to be easily kept in sync with other elements of the
configuration (Nginx also needs to know it).  If the application is configured with JSON, or even
YAML, then no conversion is required.  An example of that is the default Cassandra configuration
file held in the <code>conf</code> field of <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/cassandra.jsonnet">lib/cassandra.jsonnet</a></tt>.</p>

<p>Looking back at <tt>appserv.packer.json</tt> and <tt>tilegen.packer.json</tt> in
<tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt>, while both use Nginx/uWSGI/Flask there are some subtle differences.  Since
the HTTP request handlers are different in each case, the module (uWSGI entrypoint) and also
required packages are different.  Firstly, the tile generation image needs a provisioner to build
the C++ code.  Secondly, since the application server talks to the Cassandra database using
cassandra-driver, which interferes with pre-forking, it was necessary to override the
<code>lazy</code> field of the uWSGI configuration in that image.  This is an example of how an
abstract template can unify two similar parts of the configuration, while still allowing the
overriding of small details as needed.  Note also that such precise manipulation of configuration
details would be much harder if the uWSGI configuration was represented as a single string instead
of as a structure within the object model.</p>

<pre class="medium"><code>"appserv.packer.json": $.MyFlaskImage {
    name: "appserv-v20141222-0300",
    module: "main",   // Entrypoint in the Python code.
    pipPackages +: ["httplib2", "cassandra-driver", "blist"],
    uwsgiConf +: { lazy: "true" },  // cassandra-driver does not survive fork()
    ...
},
</code></pre>

<p>Going up to the top of the template hierarchy we have <code>GcpDebianImage</code> and finally
<code>GcpImage</code> in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/packer.jsonnet">lib/packer.jsonnet</a></tt>.  The latter gives the Packer builder configuration
for Google Cloud Platform, bringing out some fields to the top level (essentially hiding the
<code>builder</code> subobject).  We can hide the builder configuration because we only ever need
one builder per image.  We can support multiple cloud providers by deriving an entire new Packer
configuration at the top level, overriding as necessary to specialize for that platform.  The
<code>GcpDebianImage</code> selects the base image (Backports) and adds provisioners for apt and pip
packages.  The configuration of those provisioners (the list of installed packages, and additional
repositories / keys) is brought out to the top level of the image configuration.  By default, the
lists are empty but subobjects can override and extend them as we saw with
<tt>appserv.packer.json</tt>.</p>

<p>The actual provisioners <code>Apt</code> and <code>Pip</code> are defined further up
<tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/packer.jsonnet">lib/packer.jsonnet</a></tt>.  In their definitions, one can see how the various shell commands are built
up from the declarative lists of repositories, package names, etc.  Installing packages in a
non-interactive context requires a few extra switches and an environment variable, but the
<code>Apt</code> provisioner handles all that.  Also note how these provisioners both derive from
<code>RootShell</code> (defined right at the top of the file) because those commands need to be run
as root.</p>

<p>Note that everything discussed has been plain Jsonnet code.  It is possible to create your own
provisioners (based on these or from scratch) in order to declaratively control packer
in new ways.</p>


<h4>Terraform</h4>

<p>The remainder of <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> generates the Terraform configuration that defines all
the cloud resources that are required to run the fractal web application.  That includes the
instances that actually run the Packer images.  It also includes the resources that configure
network routing / firewalls, load balancers, etc.</p>

<p>Terraform accepts two basic syntaxes, JSON and <a href="https://github.com/hashicorp/hcl">HCL</a>
(a more concise form of JSON).  This provides a datastructure that specifies the resources required,
but the model also has a few computational features:  The structure has 'variables' which an be
resolved by <a href="http://en.wikipedia.org/wiki/String_interpolation">string interpolation</a>
within the resources.  All resources are also extended with a <tt>count</tt> parameter for creating
<i>n</i> replicas of that resource, and there is also some support for importing 'modules', i.e.
another Terraform configuration of resources (perhaps written by a third party).</p>

<p>The interpolation feature is also used to reference attributes of resources that are not known
until after deployment (i.e. cannot be known during Jsonnet execution time).  For example, the
generated IP address of a static IP resource called <tt>foo</tt> can be referenced from a string in
the definition of a resource <tt>bar</tt> using the syntax
<tt>${google_compute_address.foo.address}</tt>, which is resolved after the deployment of the
<tt>foo</tt> in time for the deployment of <tt>bar</tt>.</p>

<p>We choose to emit JSON instead of HCL, as the latter would require conversion code.  We also do
not make use of any of the Terraform language features, as Jsonnet provides similar or greater
capabilities in each of those domains, and doing it at the Jsonnet level allows integration with the
rest of the configuration.  We do, however, use Terraform interpolation syntax for resolving the
"not known until deployment" attributes.  For example, in order to configure the application server
with the host:port endpoint of the tile processing service.  Such resolution cannot be performed by
Jsonnet.</p>

<p>Going through <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt>, the function <code>zone</code> is used to statically
assign <a href="https://cloud.google.com/compute/docs/zones">zones</a> to instances on a round robin
basis.  All the instances extend from <code>FractalInstance</code>, which is parameterized by the
index <code>zone_hash</code> (it's actually just a function that takes zone_hash and returns an
instance template).  It is this index that is used to compute the zone, as can be seen in the body
of <code>FractalInstance</code>.  The zone is also also a namespace for the instance name, so when
we list the instances behind each load balancer in the <code>google_compute_target_pool</code>
object, we compute the zone for each instance there as well.</p>

<pre class="medium"><code>local zone(hash) =
    local arr = [
        "us-central1-a",
        "us-central1-b",
        "us-central1-f",
    ];
    arr[hash % std.length(arr)],
</code></pre>


<p><code>FractalInstance</code> also specifies some default API access scopes and tags, as well as
the network over which the instances communicate.  It extends <code>GcpInstance</code> from
<tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/terraform.jsonnet">lib/terraform.jsonnet</a></tt>, which brings default service account scopes, the network, and the
startup script to the top level, and provides some defaults for other parameters.</p>

<p>Back in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> we now have the instance definitions themselves.  These are
arranged by image into clusters: application server, db (Cassandra), and tile generation.  Terraform
expects the instances in the form of a key/value map (i.e. a JSON object), since it identifies them
internally with unique string names.  Thus the three clusters are each expressed as an object, and
they are joined with the object addition <code>+</code> operator.</p>

<pre class="medium"><code>google_compute_instance: {
    ["appserv" + k]: ...
    for k in [1, 2, 3]
} + {
    db1: ...,
    db2: ...,
    db3: ...,
} + {
    ["tilegen" + k]: ...
    for k in [1, 2, 3, 4]
}
</code></pre>

<p>The <tt>appserv</tt> and <tt>tilegen</tt> replicas are given using an <a
href="tutorial.html#comprehension">object comprehension</a> in which the field name and value are
computed with <code>k</code> set to each index in the given list.  The variable <code>k</code> also
ends up as an argument to <code>FractalInstance</code> and thus defines the zone of the instance.
In both cases, we also place a file <tt>/var/www/conf.json</tt>.  This is read on the instance by
the application, at startup, and used to configure the service.  In the tilegen replicas the
configuration comes from <code>ApplicationConf</code> from the top of <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt>.  In
the appserv instances, the same data is used, but extended with some extra fields.</p>

<pre class="medium"><code>resource.FractalInstance(k) {
    ...
    conf:: $.ApplicationConf {
        database_name: $.cassandraKeyspace,
        database_user: $.cassandraUser,
        database_pass: credentials.cassandraUserPass,
        tilegen: "${google_compute_address.tilegen.address}",
        db_endpoints: $.cassandraNodes,
    },
    startup_script +: [self.addFile(self.conf, "/var/www/conf.json")],
}
</code></pre>

<p>In both cases, the <a href="https://cloud.google.com/compute/docs/startupscript">startup
script</a> has added an extra line appended, computed by <code>self.addFile()</code>, a method
inherited from <code>GcpInstance</code> in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/terraform.jsonnet">lib/terraform.jsonnet</a></tt>.  Examining its definition
shows that it generates a line of bash to actually add the file:  </p>

<pre class="medium"><code>addFile(v, dest)::
    "echo %s > %s" % [std.escapeStringBash(v), std.escapeStringBash(dest)],
</code></pre>

<p>Finally, the Cassandra cluster is deployed via an explicit list of three nodes (db1, db2, db3).
We attend to them individually, firstly because bringing up a Cassandra cluster from scratch
requires one node to have a special boot strapping role, and secondly because database nodes are
stateful and therefore less 'expendable' than application or tile server nodes.  All three nodes
extend from the <code>CassandraInstance(i)</code> mixin, which is where they get their common
configuration.  As with <code>FractalInstance(i)</code>, the integer parameter is used to drive the
zone.  The bootstrap behavior of the first node is enabled by extending <code>GcpStarterMixin</code>
instead of <code>GcpTopUpMixin</code>.  The starter mixin has extra logic to initialize the
database, which we pass in the form of a <a
href="https://cassandra.apache.org/doc/cql/CQL.html">CQL</a> script to create the database and
replication factor for one of its internal tables.  There is some fancy footwork required to get
Cassandra into a stable state without exposing it to the network in a passwordless state.  All of
that is thankfully hidden behind the two re-usable mixins in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/lib/cassandra.jsonnet">lib/cassandra.jsonnet</a></tt>.</p>

<h2 id=using>Using The Configuration</h2>

<p>The rest of this article gives simple methodologies for deploying and managing the fractal
application by editting the Jsonnet configuration and then applying these changes.  In order to
reproduce this case study there are a few pre-requisites:</p>

<ul>

<li>A Linux or OSX system with GNU Make</li>

<li>Packer, built from github, in your $PATH.</li>

<li>Terraform, including <a href="https://github.com/hashicorp/terraform/pull/588">PR #588</a>, in
your $PATH.</li>

<li>Jsonnet, built from github (this is also where you find the configuration and source code).</li>

<li>An account on Google Cloud Platform (hosting the application will incur charges).</li>

</ul>

<p>Once those are satisfied, follow these steps:</p>

<ol>

<li>In the Google Cloud Platform console, open your project, go to APIs and Auth / credentials, and
create a new service account.  This will automatic download a p12 key, which you can delete as we
will not be using it.  Instead, click the button to download a JSON key for the new service account,
move it to the fractal directory, and call it <tt>service_account_key.json</tt>.</li>

<li>Create a credentials.jsonnet file based on the template, fill in your GCP project name and make
up some unique passwords.</li>

</ol>

<h3 id=using_deploy>Initial Deployment and Tear Down</h3>

<p>To deploy the application, run make -j.  This should start running 3 Packer builds in parallel.
In a separate terminal, use tail -f *.log in order to watch their progress.  When the images are
built, Terraform will show you the proposed changes (a long list of resources to be created).  Enter
<tt>y</tt> to confirm the changes.  In time, the application will be deployed.  Now you only need
the appserv ip address to connect to it.  You can get this using "gcloud compute addresses list" or
by navigating the Google Cloud Platform console to "networks".  Opening that ip in a web browser
should take you to the fractal application itself.  </p>

<p>The application can be brought down again by running <tt>terraform destroy</tt>.  Terraform
remembers the resources it created via the <tt>terraform.tfstate</tt> file.  This will not destroy
the Packer images, they can be deleted from the console or from gcloud.</p>

<p>Managing a production web service usually means making continual changes to it instead of
bringing the whole thing down and up again, as we will shortly discuss.  However it is still useful
to bring up a fresh application for testing / development purposes.  A copy or variant of the
production service can be brought up concurrently with the production service (e.g. in a different
project).  This can be useful for QA, automatic integration testing, or load testing.  It is also
useful for training new ops staff or rehearsing a complex production change in a safe
environment.</p>

<h3 id=using_cassandra>Add / Remove Cassandra Nodes</h3>

<p>Managing the Cassandra cluster requires a combination of configuration alteration (to control the
fundamental compute resources) and use of the Cassandra commandline tool "nodetool" on the instances
themselves.  For example "nodetool status fractal" on any Cassandra instance will give information
about the whole cluster.</p>

<p>To add a new node (expand the cluster), simply edit <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt>, add another
instance in the Terraform configuration and run make -j.  Confirm the changes (the only change
should be the new instance, e.g. db4).  It will start up and soon become part of the cluster.</p>

<p>To remove a node, first decommission it using nodetool -h HOSTNAME decommission.  When that is
complete, destroy the actual instance by updating <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> to remove the resource
and run make -j again.  Confirm the removal of the instance.  It is OK to remove the first node, but
its replacement should use <tt>GcpTopUpMixin</tt> instead of <tt>GcpStarterMixin</tt>.  You can
recycle all of the nodes if you do it one at a time, which is actually necessary for emergency
kernel upgrades.</p>

<p>If a node is permanently and unexpectedly lost (e.g. a disk error), or you removed it without
first decommissioning it, the cluster will remain in a state where it expects the dead node to
return at some point (as if it were temporarily powered down or on the wrong side of a network
split).  This situation an be rectified with nodetool removenode UUID, run from any other node in
the cluster.  In this case it is probably also necessary to run nodetool repair on the other nodes
to ensure data is properly distributed.</p>


<h3 id=using_appserv>Canary A Change To The Application Server</h3>

<p>To introduce new functionality to the application server it is useful to divert a small
proportion of user traffic to the new code to ensure it is working properly.  After this initial
"canary" test has passed, the remaining traffic can then be confidently transferred to the new code.
The same can be said for the tile generation service (e.g. to update the C++ code).</p>

<p>The model used by this example is that the application server logic and static content are
embedded in the application server image.  Canarying consists of building a new image and then
rolling it out gradually one instance at a time.  Each step of this methodology consists of a small
modification to <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> and then running make -j.</p>

<ol>

<li>Edit the <tt>appserv.packer.json</tt> packer configuration in <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt> to update
the date embedded in the <tt>name</tt> field to the current date, and also make any desired changes
to the configuration of the image or any of the referenced Python / HTML / CSS files.</li>

<li>Run make -j to build the new image.  Note that the previous image is still available under the
old name, which means it is possible to create new instances using either the old or new image.
This feature is essential to allow ongoing maintenance of the cluster if the change is rolled
back.</li>

<li>Create a single instance with the new image by adding it to the <tt>google_compute_instance</tt>
section of <tt><a href="https://github.com/google/jsonnet/blob/master/case_studies/fractal/service.jsonnet">service.jsonnet</a></tt>.  The easiest way to do this is to copy-paste the existing
definition, and modify the image name in the copy to reflect the new image.  This allows easily
rolling back by deleting the copied code, and you can also transition the rest of the nodes by
deleting the original copy.  Thus the duplication is only temporary.  At this point the
configuration may look like this:

<pre class="medium"><code>google_compute_instance: {

    ["appserv" + k]: resource.FractalInstance(k) {
        name: "appserv" + k,
        image: "appserv-v20141222-0300",
        ...
    }
    for k in [1, 2, 3]

} + {

    ["appserv" + k]: resource.FractalInstance(k) {
        name: "appserv" + k,
        image: "appserv-v20150102-1200",
        ...
    }
    for k in [4]

} + ...
</code></pre>

Also modify the appserv target pool to add the new instance 4, thus ensuring it receives
traffic.</li>

<pre class="medium"><code>appserv: {
    name: "appserv",
    health_checks: ["${google_compute_http_health_check.fractal.name}"],
    instances: [ "%s/appserv%d" % [zone(k), k] for k in [1, 2, 3, 4] ],
},
</code></pre>
</li>

<li>Run make -j to effect those changes, and monitor the situation to ensure that there is no spike
in errors.  It is also possible to run make -j between the above two steps if it is desired to
interact with the new instance before directing user traffic at it.</li>

<li>If there is a problem, pull 4 out of the target pool and re-run make -j.  That will leave the
instance up (useful for investigation) but it will no-longer receive user traffic.  Otherwise, add
more instances (5, 6, ...) and add them to the target pool.  You can now start pulling the old
instances out of the target pool ensuring that there is always sufficient capacity for your traffic
load.  As always, make -j punctuates the steps.</li>

<li>Once the old instances are drained of user traffic, they can be destroyed.  You can do this in
batches or one at a time.  Eventually the configuration will look like this, at which point the
first block no-longer contributes anything to the configuration and it can be deleted. 

<pre class="medium"><code>google_compute_instance: {

    ["appserv" + k]: resource.FractalInstance(k) {
        name: "appserv" + k,
        image: "appserv-v20141222-0300",
        ...
    }
    for k in []

} + {

    ["appserv" + k]: resource.FractalInstance(k) {
        name: "appserv" + k,
        image: "appserv-v20150102-1200",
        ...
    }
    for k in [4, 5, 6]

} + ...
</code></pre>
</li>

</ol>

<h2 id=conclusion>Conclusion</h2>

<p>We have shown how Jsonnet can be used to centralize, unify, and manage configuration for a
realistic cloud application.  We have demonstrated how programming language abstraction techniques
make the configuration very concise, with re-usable elements separated into template libraries.
Complexity is controlled in spite of the variety of different configurations, formats, and tasks
involved/</p>

<p>Finally we demonstrated how with a little procedural glue to drive other processes (the humble
UNIX make), we were able to build an operations methodology where many aspects of the service can be
controlled centrally by editing a single Jsonnet file and issuing make -j update commands.</p>

<div style="margin-bottom: 50px"></div>
<hr />
<p class="copyright">
Except as noted, this content is licensed under Creative Commons Attribution 2.5.
</p>
</body>

</html>